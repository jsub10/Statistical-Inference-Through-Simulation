{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsubapple/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsubapple/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "# Use the functions from another notebook in this notebook\n",
    "%run SharedFunctions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostic Tests\n",
    "\n",
    "In the *From_Probability_to_Statistical_Inference* notebook we learned about the classical statistics recipe for accepting or rejecting a hypothesis. For example, we assumed a coin had a bias of, say, 0.3 towards heads (the hypothesis), and calculated the p-value of the hypothesis based on evidence that consisted of the number of heads observed when the coin was tossed a certain number of times. We also looked at how we could actually get what we needed -- the probability of the *hypothesis* given the evidence. Recall that with the classical p-value recipe, we just skirted this calculation and instead kept calculating the probability of the *evidence* given the hypothesis and then tried to make the most of it. Bayes rule gives us the first probability and it's the one we really need.\n",
    "\n",
    "In the notebook titled *Why_Significance_Tests_Are_Useless* we saw all of the reasons for *not* doing things the p-value way. However, this technique continues to be widely used for prediction and prediction has a number of concepts associated with it that form the core of literacy in statistics. In this notebook we'll cover these concepts.\n",
    "\n",
    "The central concept we'll talk about is the *diagnostic test*. Think of this as a prediction; to make it concrete it can be the prediction that a coin has bias 0.6 towards heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Diagnostic Test Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a prediction table\n",
    "def createPredTable(colNames, cellVals):\n",
    "    # len(colNames) should be the same as the length of each element in cellVals\n",
    "    for cellVal in cellVals:\n",
    "        if len(colNames) != len(cellVal):\n",
    "            return \"Please ensure that every element of cellVals is the same length as the length of colNames\"\n",
    "        \n",
    "    predTable = tabulate(cellVals, colNames, tablefmt = \"grid\")\n",
    "    \n",
    "    return predTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------+---------------------+\n",
      "|               | Is Really X         | Is Really NOT X     |\n",
      "+===============+=====================+=====================+\n",
      "| Predict X     | True Positive (TP)  | False Positive (FP) |\n",
      "+---------------+---------------------+---------------------+\n",
      "| Predict NOT X | False Negative (FN) | True Negative (TN)  |\n",
      "+---------------+---------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "# Create a 2x2 prediction table\n",
    "colNames = [\"\", \"Is Really X\", \"Is Really NOT X\"]\n",
    "cellNames = [[\"Predict X\", \"True Positive (TP)\", \"False Positive (FP)\"], \n",
    "             [\"Predict NOT X\", \"False Negative (FN)\", \"True Negative (TN)\"]]\n",
    "predTable = tabulate(cellNames, colNames, tablefmt = \"grid\")\n",
    "print predTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we may not know if our prediction is true of false. Sometimes we can get this information. For example, the p-value recipe gives us a diagnostic test for whether a coin has a certain bias, let's say 0.6 towards heads. Suppose I manufacture a device for testing if the coin has a bias of 0.6. As a manufacturer of this device, I owe it to you to have tested several coins with known bias of 0.6 and give you a table with numbers that look like the following. That's what you should expect from the manufacturer of any diagnostic test whether it be a pregnancy test or a test to sift out terrorists from non-terrorists in an airport check in line.\n",
    "\n",
    "The prediction table for our coin-bias device could look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+----------------------+--------------------------+\n",
      "|                         |   Bias Really Is 0.6 |   Bias Really Is NOT 0.6 |\n",
      "+=========================+======================+==========================+\n",
      "| Predict Bias is 0.6     |                  493 |                        7 |\n",
      "+-------------------------+----------------------+--------------------------+\n",
      "| Predict Bias is NOT 0.6 |                  307 |                      193 |\n",
      "+-------------------------+----------------------+--------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(createPredTable([\"\", \"Bias Really Is 0.6\", \"Bias Really Is NOT 0.6\"],\n",
    "                [[\"Predict Bias is 0.6\", 493, 7], \n",
    "                 [\"Predict Bias is NOT 0.6\", 307, 193]\n",
    "                ]\n",
    "               )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power and Significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the numbers that the manufacturer of the diagnostic test *owes* us. If you manufacture a device that classifies things into two or more types, you have the responsibility to produce such a table so your customers can make an informed decision about how good your device is. \n",
    "\n",
    "In this case the test's manufacturer has put 800 coins (493 + 307) *with known bias of 0.6* through their device and found that the device correctly identified  493 of the coins as having a bias of 0.6. However, the device *misidentified* 307 coins by predicting that their bias is not 0.6 while indeed they were. So the manufacturer has given us the True Positive rate - 493/800. To write this out more fully,\n",
    "\n",
    "- $P(predict\\ X\\ |\\ X) = TP/(TP + FN) = 493/800$ a.k.a. True Positive Rate, $1-\\beta$, Power, Sensitivity\n",
    "- $P(predict\\ NOT\\ X\\ |\\ X) = FN/(TP + FN) = 307/800$ a.k.a. False Negative Rate, $\\beta$\n",
    "- $P(predict\\ X\\ |\\ NOT\\ X) = FP/(FP + TN) = 7/200$ a.k.a. False Positive Rate, $\\alpha$, Significance Level\n",
    "- $P(predict\\ NOT\\ X\\ |\\ NOT\\ X) = TN/(FP + TN) = 193/200$ a.k.a. True Negative Rate, $1-\\alpha$, Selectivity\n",
    "\n",
    "Put aside how the manufacturer knows the biases of the coins - but keep in mind that they would *have to* have such a known set of coins in order to calibrate their device. It seems like this device we have is very good at saying when the bias is *not* 0.6 but does quite badly in indentifying the coins that *do* have a bias of 0.6.\n",
    "\n",
    "Right away, we can see that there's a tight relationship between the True Positive Rate and the False Negative Rate. If we start with a fixed number of coins of known bias of 0.6, then as the True Positive Rate goes up the False Negative Rate *must* come down (and vice versa). Similarly, when the False Positive Rate goes up, the True Negative rate *must* come down (and vice versa). Mathematical consistency demands it.\n",
    "\n",
    "We can also see that there is no mathematical relationship between the True Positive Rate and the True Negative Rate -- knowing something about one tells us nothing about the other. If I told you that my device had high Sensitivity, it's anyone's guess what it's Selectivity is going to be -- that depends on the details of how the device goes about doing what it does. There is no mathematical relationship between them. It would be great to build the device to have high rates for both Sensitivity and Selectivity.\n",
    "\n",
    "Similarly, there is no mathematical relationship betweeen the False Positive Rate and the False Negative Rate -- a device could be great at one and terrible at the other. (Can it be terrible at both? Great at both?)\n",
    "\n",
    "We can think of the p-value recipe for determining whether a coin has a certain bias as a kind of detector. If you remember, the significance level determines the threshold p-value for rejecting the hypothesis. A threshold of 0.05 means that the hypothesis will be falsely rejected 5% of the time -- in other words, this is the false positive rate. The true negative rate would then be 95%.\n",
    "\n",
    "\"A big difference between power and $\\alpha$ is that $\\alpha$ is something you just decide on. Power ... depends on variation and sample size. Variation is a natural part of the world and you can't change it, but you can choose a sample size for your study.\" (Vickers, p.73) \n",
    "\n",
    "\n",
    "\n",
    "When we build a detector, a device that conducts a diagnostic test, we are acutely aware of the detector's intended purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Results of Diagnostic Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we really want is not $P(predict\\ X\\ |\\ X)$ but $P(X\\ |\\ predict\\ X)$ \n",
    "\n",
    "accuracy = (tp + tn)/tp + tn + fp + fn (Of all the things we identified (both positive and negative), which ones were correctly identified? It's not a completely vacuous metric -- but it doesn't tell the whole story. I can game this metric by ... )\n",
    "\n",
    "precision = tp/(tp + fp)\n",
    "\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "F1 score = (2 * precision * recall)/(precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "to continue..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
